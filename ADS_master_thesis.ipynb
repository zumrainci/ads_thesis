{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install python-louvain"
      ],
      "metadata": {
        "id": "HeZrUBJbs6Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RNIRFS6ZnEUc"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "import time\n",
        "from scipy.stats import chi2_contingency\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import community.community_louvain as community_louvain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "I0mhPFTgs5K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read all members csv\n",
        "df = pd.read_excel(\"path/to/data.xlsx\")"
      ],
      "metadata": {
        "id": "tYAmLyEinR1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the dataset and preprocess"
      ],
      "metadata": {
        "id": "SC71cL-WxC-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hs-avpUfxCGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "T4UOSoGCY5eZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant fields for network analysis\n",
        "fields_to_keep = [\n",
        "    'linkedinProfileUrl',\n",
        "    'fullName',\n",
        "    'companyName',\n",
        "    'linkedinJobTitle',\n",
        "    'linkedinDescription',\n",
        "    'location',\n",
        "    'linkedinJobLocation',\n",
        "    'companyIndustry',\n",
        "    'linkedinHeadline',\n",
        "    'linkedinSkillsLabel',\n",
        "    'linkedinSchoolName'\n",
        "]\n",
        "\n",
        "# Keep only these columns\n",
        "df = df[fields_to_keep].copy()"
      ],
      "metadata": {
        "id": "evkQqQReneYz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the null value count of all columns\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "jGcPYkzzwJBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop location nulls\n",
        "df = df.dropna(subset=['location'])\n",
        "\n",
        "# Drop job location column\n",
        "df = df.drop(columns=['linkedinJobLocation'])"
      ],
      "metadata": {
        "id": "L3miLd2Pu0f1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Location transformation to country-level"
      ],
      "metadata": {
        "id": "xwKw2CJjuTg-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4XyHLYfnXqWL"
      },
      "outputs": [],
      "source": [
        "def clean_location_text(loc):\n",
        "    \"\"\"\n",
        "    This function cleans location text by removing emojis,\n",
        "    special characters, and the term 'International'.\n",
        "    \"\"\"\n",
        "    if pd.isna(loc):\n",
        "        return \"\"\n",
        "\n",
        "    loc = re.sub(r'[^\\w\\s,;-]', '', str(loc))  # Remove emojis and special characters\n",
        "    loc = re.sub(r'\\bInternational\\b', '', loc, flags=re.IGNORECASE)\n",
        "\n",
        "    return loc.strip()\n",
        "\n",
        "df['location'] = df['location'].apply(clean_location_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "geolocator = Nominatim(user_agent=\"culttech-thesis\")\n",
        "\n",
        "def get_country(location):\n",
        "    if not location:\n",
        "        return \"\"\n",
        "    try:\n",
        "        geo = geolocator.geocode(location, language='en', addressdetails=True, timeout=10)\n",
        "        time.sleep(1)  # Respect API rate limit\n",
        "        if geo and 'country' in geo.raw['address']:\n",
        "            return geo.raw['address']['country']\n",
        "    except GeocoderTimedOut:\n",
        "        return \"\"\n",
        "    return \"\"\n",
        "\n",
        "df['location'] = df['location'].apply(get_country)"
      ],
      "metadata": {
        "id": "vxBUkF80xzim"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['location'].head()"
      ],
      "metadata": {
        "id": "Wp7DehQ4z320"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6hUJwhffTPR"
      },
      "outputs": [],
      "source": [
        "df['location'] = df['country']\n",
        "df = df.drop(columns=['country'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorization"
      ],
      "metadata": {
        "id": "2ADtePM5n6_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "custom_phrases = ['microsoft excel', 'microsoft office', 'microsoft', 'microsoft powerpoint', 'powerpoint', 'office']\n",
        "role_words = {\n",
        "    'ceo', 'founder', 'cofounder', 'director', 'manager',\n",
        "    'owner', 'partner', 'president', 'leader', 'head',\n",
        "    'executive', 'chief', 'officer', 'consultant', 'staff',\n",
        "    'managing', 'chairman', 'employee'}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Applies a series of text preprocessing steps:\n",
        "    1. Lowercasing\n",
        "    2. URL removal\n",
        "    3. Custom phrase removal\n",
        "    4. Punctuation and number removal\n",
        "    5. Tokenization\n",
        "    6. Stop word removal\n",
        "    7. Lemmatization\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|gov|edu|io|co|uk|de|fr|jp|cn|in|ru|br|au|ca|ch|es|it|nl|se|no|dk|fi|ie|nz|at|be|gr|hk|il|kr|mx|my|ph|sg|th|tw|vn|za)\\S*'\n",
        "    text = re.sub(url_pattern, '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove custom phrases\n",
        "    for phrase in custom_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenization, stop word removal, and lemmatization\n",
        "    tokens = []\n",
        "    if text.strip():\n",
        "        for word in text.split():\n",
        "            if word not in stop_words and word not in role_words:\n",
        "              tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "GK6A_liy0v3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine which text field combination to use"
      ],
      "metadata": {
        "id": "IA8K-KTC1MTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define candidate text columns\n",
        "candidate_text_columns = [\n",
        "    'linkedinJobTitle',\n",
        "    'linkedinDescription',\n",
        "    'linkedinSkillsLabel',\n",
        "    'linkedinHeadline',\n",
        "    'companyIndustry'\n",
        "]\n",
        "\n",
        "# Iterate through combinations and evaluate clustering\n",
        "results = []\n",
        "max_k_to_test = 10\n",
        "\n",
        "# Loop through combinations of all fields\n",
        "for i in range(1, len(candidate_text_columns) + 1):\n",
        "    for combo in itertools.combinations(candidate_text_columns, i):\n",
        "        current_columns = list(combo)\n",
        "        combo_name = \" + \".join(current_columns)\n",
        "\n",
        "        # Concatenate columns for the current combination\n",
        "        df['current_text'] = df[current_columns].fillna('').agg(' '.join, axis=1)\n",
        "        df['current_cleaned_text'] = df['current_text'].apply(preprocess_text)\n",
        "\n",
        "        if df['current_cleaned_text'].str.strip().eq('').all():\n",
        "            print(f\"Skipping combination '{combo_name}' as cleaned text is empty.\")\n",
        "            continue\n",
        "\n",
        "        # TF-IDF\n",
        "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=5)\n",
        "        tfidf_matrix = vectorizer.fit_transform(df['current_cleaned_text'])\n",
        "\n",
        "        best_silhouette = -1 # Silhouette scores range from -1 to 1\n",
        "        best_wcss = float('inf')\n",
        "        optimal_k = -1\n",
        "\n",
        "        # Evaluate K from 2 to max_k_to_test\n",
        "        for k in range(2, max_k_to_test + 1):\n",
        "            if k > tfidf_matrix.shape[0]:\n",
        "                break\n",
        "\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "            cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "            # Check if there's only one cluster formed (can happen with very sparse data)\n",
        "            if len(set(cluster_labels)) < 2:\n",
        "                continue\n",
        "\n",
        "            current_silhouette = silhouette_score(tfidf_matrix, cluster_labels)\n",
        "            current_wcss = kmeans.inertia_\n",
        "\n",
        "            if current_silhouette > best_silhouette:\n",
        "                best_silhouette = current_silhouette\n",
        "                best_wcss = current_wcss\n",
        "                optimal_k = k\n",
        "\n",
        "        if optimal_k != -1: # Only store results if at least one valid K was found\n",
        "            results.append({\n",
        "                'Combination': combo_name,\n",
        "                'Optimal K': optimal_k,\n",
        "                'Max silhouette score': best_silhouette,\n",
        "                'WCSS at optimal K': best_wcss\n",
        "            })\n",
        "\n",
        "print(\"Text fields combination evaluation results\")\n",
        "results_df = pd.DataFrame(results).sort_values(by='Max silhouette score', ascending=False)\n",
        "\n",
        "# Save results_df as CSV\n",
        "results_df.to_csv(\"text_fields_combination_evaluation_results.csv\", index=False)\n",
        "print(results_df.to_markdown(index=False))"
      ],
      "metadata": {
        "id": "cDxIYvj9S46i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply clustering to selected fields"
      ],
      "metadata": {
        "id": "GGRyVTZ11Puj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare 'text' column\n",
        "text_columns = [\n",
        "    'linkedinDescription',\n",
        "    'linkedinJobTitle',\n",
        "    'companyIndustry',\n",
        "]\n",
        "\n",
        "# Combine into one text field\n",
        "df['text'] = df[text_columns].fillna('').agg(' '.join, axis=1)\n",
        "\n",
        "# Apply the preprocessing function to create the 'cleaned_text' column\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "JH7unp7QcgLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_text'].head()"
      ],
      "metadata": {
        "id": "L2nubaKGd6FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "# max_features limits the vocabulary size to the top 5000 most frequent terms\n",
        "# ngram_range=(1,2) includes single words (unigrams) and two-word phrases (bigrams)\n",
        "# min_df=5 ignores terms that appear in less than 5 documents\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=5)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Plot elbow method\n",
        "wcss = []\n",
        "max_k_for_plots = 15\n",
        "for i in range(1, max_k_for_plots + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42, n_init='auto')\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "    wcss.append(kmeans.inertia_) # inertia_ is the WCSS\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, max_k_for_plots + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow method for optimal K')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "plt.xticks(range(1, max_k_for_plots + 1))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot silhouette score\n",
        "silhouette_scores = []\n",
        "for i in range(2, max_k_for_plots + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42, n_init='auto')\n",
        "    cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "    # Check if there's only one cluster formed (can happen with very sparse data)\n",
        "    if len(set(cluster_labels)) < 2:\n",
        "        # Silhouette score is not defined for a single cluster\n",
        "        silhouette_scores.append(0)\n",
        "        continue\n",
        "\n",
        "    score = silhouette_score(tfidf_matrix, cluster_labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(2, max_k_for_plots + 1), silhouette_scores, marker='o', linestyle='--')\n",
        "plt.title('Silhouette score for optimal K')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.xticks(range(2, max_k_for_plots + 1))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ZqRovmFd5Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final K-Means clustering with optimal K\n",
        "n_clusters = 10\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
        "clusters = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "df['cluster'] = clusters\n",
        "\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"\\nTop terms per cluster:\")\n",
        "for i in range(n_clusters):\n",
        "    cluster_members_count = sum(df['cluster'] == i)\n",
        "    print(f\"\\nCluster {i} (Members: {cluster_members_count}):\")\n",
        "    top_terms = [terms[ind] for ind in order_centroids[i, :15]]\n",
        "    print(f\"  Top terms: {', '.join(top_terms)}\")\n",
        "\n",
        "    # Generate word cloud for the current cluster\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(top_terms))\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Word Cloud for Cluster {i}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lxH6_iaJeW7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually assign cluster labels\n",
        "cluster_labels = {\n",
        "    0: \"Public sector & economic development\",\n",
        "    1: \"Creative & design\",\n",
        "    2: \"Management consulting\",\n",
        "    3: \"Venture capital & investment\",\n",
        "    4: \"Music & media production\",\n",
        "    5: \"Finance, IT & business services\",\n",
        "    6: \"Museums & cultural institutions\",\n",
        "    7: \"Tech & software\",\n",
        "    8: \"General startup & tech ecosystem\",\n",
        "    9: \"Academia & higher education\"\n",
        "}\n",
        "\n",
        "# Add labels to the DataFrame\n",
        "df['cluster_label'] = df['cluster'].map(cluster_labels)\n",
        "\n",
        "# View value counts of each cluster\n",
        "print(df[['cluster', 'cluster_label']].value_counts())"
      ],
      "metadata": {
        "id": "u0yI046JiTvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the frequencies of cluster_label column\n",
        "df['cluster_label'].value_counts().plot(kind='bar', figsize=(15,7), color='skyblue')\n",
        "\n",
        "plt.title('Frequency of cluster labels')\n",
        "plt.xlabel('Cluster label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qQejTjQXFOAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a cleaner DataFrame to construct the network and assign node IDs"
      ],
      "metadata": {
        "id": "psQTxKxO1guF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_keep = [\n",
        "    'companyName',\n",
        "    'location',\n",
        "    'linkedinSchoolName',\n",
        "    'cluster_label'\n",
        "    ]\n",
        "df = df[column_to_keep]\n",
        "df['node_id'] = ['S' + str(i + 1) for i in range(len(df))]"
      ],
      "metadata": {
        "id": "7A2YdxsfkOlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "1zB1fYX7klhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_df = df[['node_id', 'fullName', 'cluster_label', 'companyName', 'linkedinSchoolName', 'location']]\n",
        "nodes_df.head()\n",
        "\n",
        "# Save nodes\n",
        "nodes_df.to_csv(\"nodes.csv\", index=False)"
      ],
      "metadata": {
        "id": "_XSPhq0flUDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Cramer's V for categorical variables"
      ],
      "metadata": {
        "id": "9WrHdM9dB8kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['companyName', 'location', 'linkedinSchoolName', 'cluster_label']\n",
        "\n",
        "# Function to compute Cramér's V for categorical variables\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    if confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
        "        return np.nan\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k - 1)*(r - 1)) / (n - 1))\n",
        "    rcorr = r - ((r - 1)**2) / (n - 1)\n",
        "    kcorr = k - ((k - 1)**2) / (n - 1)\n",
        "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = pd.DataFrame(index=categorical_cols, columns=categorical_cols)\n",
        "\n",
        "for col1 in categorical_cols:\n",
        "    for col2 in categorical_cols:\n",
        "        correlation_matrix.loc[col1, col2] = cramers_v(nodes_df[col1], nodes_df[col2])\n",
        "\n",
        "correlation_matrix = correlation_matrix.astype(float)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Cramér's V correlation matrix for categorical attributes\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2jVE8ubQHJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge construction"
      ],
      "metadata": {
        "id": "QtPOnqLxCdA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges = []\n",
        "for a, b in combinations(nodes_df.itertuples(index=False), 2):\n",
        "    strong_match = False\n",
        "\n",
        "    if a.companyName == b.companyName and pd.notnull(a.companyName):\n",
        "        strong_match = True\n",
        "    elif a.linkedinSchoolName == b.linkedinSchoolName and pd.notnull(a.linkedinSchoolName):\n",
        "        strong_match = True\n",
        "\n",
        "    if strong_match:\n",
        "        edges.append({'source': a.node_id, 'target': b.node_id})\n",
        "    else:\n",
        "        if a.cluster_label == b.cluster_label:\n",
        "            if a.location == b.location and pd.notnull(a.location):\n",
        "                edges.append({'source': a.node_id, 'target': b.node_id})  # Add soft edge\n",
        "\n",
        "# Save edges\n",
        "edges_df = pd.DataFrame(edges)\n",
        "edges_df.to_csv(\"edges.csv\", index=False)"
      ],
      "metadata": {
        "id": "TjXK3JicVQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network construction"
      ],
      "metadata": {
        "id": "ZKGaL9JPC-Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes with attributes\n",
        "for _, row in nodes_df.iterrows():\n",
        "    G.add_node(\n",
        "        row['node_id'],\n",
        "        cluster_label=row['cluster_label'],\n",
        "        companyName=row['companyName'],\n",
        "        linkedinSchoolName=row['linkedinSchoolName'],\n",
        "        location=row['location']\n",
        "    )\n",
        "\n",
        "# Add edges\n",
        "for _, row in edges_df.iterrows():\n",
        "    G.add_edge(row['source'], row['target'])"
      ],
      "metadata": {
        "id": "1Mag6nDaTRUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate positions\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Assign positions to nodes\n",
        "for node in G.nodes():\n",
        "    G.nodes[node]['pos'] = pos[node]"
      ],
      "metadata": {
        "id": "QXL4zGhWTrt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create edge traces\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.extend([x0, x1, None])\n",
        "    edge_y.extend([y0, y1, None])\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines'\n",
        ")"
      ],
      "metadata": {
        "id": "dHWdO0MITuXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map cluster labels to numeric codes for coloring\n",
        "cluster_labels = [G.nodes[node]['cluster_label'] for node in G.nodes()]\n",
        "label_to_int = {label: i for i, label in enumerate(sorted(set(cluster_labels)))}\n",
        "\n",
        "# Create node traces\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "node_color = []\n",
        "\n",
        "for node in G.nodes():\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    cluster_label = G.nodes[node]['cluster_label']\n",
        "    node_color.append(label_to_int[cluster_label])\n",
        "    node_text.append(\n",
        "        f\"Node ID: {node}<br>\"\n",
        "        f\"Company: {G.nodes[node]['companyName']}<br>\"\n",
        "        f\"School: {G.nodes[node]['linkedinSchoolName']}<br>\"\n",
        "        f\"Location: {G.nodes[node]['location']}<br>\"\n",
        "        f\"Sector: {cluster_label}\"\n",
        "    )\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    text=node_text,\n",
        "    marker=dict(\n",
        "        size=10,\n",
        "        color=node_color,\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(\n",
        "            title='Cluster Label (Encoded)'\n",
        "        ),\n",
        "        line=dict(width=1)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "2xHBuRXSgqvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Culttech members network',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20, l=20, r=20, t=40),\n",
        "                    xaxis=dict(\n",
        "                        showgrid=False,\n",
        "                        zeroline=False,\n",
        "                        #scaleanchor='y',\n",
        "                        #scaleratio=1\n",
        "                    ),\n",
        "                    yaxis=dict(\n",
        "                        showgrid=False,\n",
        "                        zeroline=False\n",
        "                    )\n",
        "                ))\n",
        "\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ctqfxW2BViXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate node-level network measures"
      ],
      "metadata": {
        "id": "lIEreEtLDuUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree centrality\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "# Betweenness centrality\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Closeness centrality\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "\n",
        "# Eigenvector centrality\n",
        "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)"
      ],
      "metadata": {
        "id": "-TVZdB6CiFdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centrality_df = pd.DataFrame({\n",
        "    'node_id': list(degree_centrality.keys()),\n",
        "    'degree_centrality': list(degree_centrality.values()),\n",
        "    'betweenness_centrality': list(betweenness_centrality.values()),\n",
        "    'closeness_centrality': list(closeness_centrality.values()),\n",
        "    'eigenvector_centrality': list(eigenvector_centrality.values())\n",
        "})\n",
        "\n",
        "centrality_df = centrality_df.merge(nodes_df, on='node_id')\n",
        "\n",
        "top_degree = centrality_df.nlargest(10, 'degree_centrality')\n",
        "top_betweenness = centrality_df.nlargest(10, 'betweenness_centrality')\n",
        "top_closeness = centrality_df.nlargest(10, 'closeness_centrality')\n",
        "top_eigenvector = centrality_df.nlargest(10, 'eigenvector_centrality')\n",
        "\n",
        "print(\"Top 10 by Degree Centrality:\\n\", top_degree[['node_id', 'degree_centrality']].reset_index(drop=True))\n",
        "print(\"\\nTop 10 by Betweenness Centrality:\\n\", top_betweenness[['node_id', 'betweenness_centrality']].reset_index(drop=True))\n",
        "print(\"\\nTop 10 by Closeness Centrality:\\n\", top_closeness[['node_id', 'closeness_centrality']].reset_index(drop=True))\n",
        "print(\"\\nTop 10 by Eigenvector Centrality:\\n\", top_eigenvector[['node_id', 'eigenvector_centrality']].reset_index(drop=True))\n",
        "\n",
        "# Save results to CSV\n",
        "top_degree.to_csv(\"top_degree_centrality.csv\", index=False)\n",
        "top_betweenness.to_csv(\"top_betweenness_centrality.csv\", index=False)\n",
        "top_closeness.to_csv(\"top_closeness_centrality.csv\", index=False)\n",
        "top_eigenvector.to_csv(\"top_eigenvector_centrality.csv\", index=False)"
      ],
      "metadata": {
        "id": "gE09mNy7ijv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network-level measures"
      ],
      "metadata": {
        "id": "8w4XU1i4J6k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of nodes and edges\n",
        "num_nodes = G.number_of_nodes()\n",
        "num_edges = G.number_of_edges()\n",
        "\n",
        "# Network density\n",
        "density = nx.density(G)\n",
        "\n",
        "# Largest connected component\n",
        "if nx.is_connected(G):\n",
        "    diameter = nx.diameter(G)\n",
        "    avg_path_length = nx.average_shortest_path_length(G)\n",
        "else:\n",
        "    largest_cc_nodes = max(nx.connected_components(G), key=len)\n",
        "    largest_cc = G.subgraph(largest_cc_nodes).copy()\n",
        "    diameter = nx.diameter(largest_cc)\n",
        "    avg_path_length = nx.average_shortest_path_length(largest_cc)\n",
        "\n",
        "# Average clustering coefficient\n",
        "avg_clustering = nx.average_clustering(G)\n",
        "\n",
        "print(\"Network-level measures\")\n",
        "print(f\"Number of nodes: {num_nodes}\")\n",
        "print(f\"Number of edges: {num_edges}\")\n",
        "print(f\"Density: {density:.4f}\")\n",
        "print(f\"Diameter (largest connected component): {diameter}\")\n",
        "print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
        "print(f\"Average shortest path length (LCC): {avg_path_length:.4f}\")"
      ],
      "metadata": {
        "id": "iEcOeUGiJ9pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Louvain community detection algorithm"
      ],
      "metadata": {
        "id": "GSRMs_SQgnZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partition = community_louvain.best_partition(G)\n",
        "\n",
        "# Add community info to graph nodes\n",
        "nx.set_node_attributes(G, partition, 'louvain_community')"
      ],
      "metadata": {
        "id": "NkDgoPLHgtp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modularity_score = community_louvain.modularity(partition, G)\n",
        "print(f\"Modularity: {modularity_score:.4f}\")"
      ],
      "metadata": {
        "id": "RXEtbh4MhjBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_communities = len(set(partition.values()))\n",
        "print(f\"Number of detected communities: {num_communities}\")"
      ],
      "metadata": {
        "id": "oPaXk8f8hkei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "community_sizes = Counter(partition.values())\n",
        "print(\"Community sizes:\", community_sizes)"
      ],
      "metadata": {
        "id": "tCvTqzC_huNR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
